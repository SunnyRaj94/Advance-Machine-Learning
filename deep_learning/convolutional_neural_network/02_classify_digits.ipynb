{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "problem statement :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We need to identify the digit in given images. We have total 70,000 images, out of which 49,000 are part of train images with the label of digit and rest 21,000 images are unlabeled (known as test images). Now, We need to identify the digit for test images. Public and Private split for test images are 40:60 and evaluation metric of this challenge is accuracy\n",
    "    https://drive.google.com/open?id=1-TavdjU2ohg5T6ZN1KUiWE9BNWu9_vhi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin3/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/admin3/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/admin3/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/admin3/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/admin3/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/admin3/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#importing necessary library\n",
    "#importing matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "#importing seaborn\n",
    "import seaborn as sea\n",
    "#importing pandas \n",
    "import pandas as pd\n",
    "#importing numpy\n",
    "import numpy as np\n",
    "\n",
    "# importing my custom library file methods\n",
    "import sys\n",
    "sys.path.append('/home/admin3/ml_with_phoenix/deep_learning/lib_and_pkl_files/')\n",
    "from ipynb.fs.full.library import *\n",
    "\n",
    "#importing job-lib\n",
    "import joblib\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Convolution2D,MaxPooling2D,Flatten\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making CNN model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/admin3/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "classifier = Sequential()\n",
    "classifier.add(Convolution2D(32,3,3,input_shape=(64,64,3),activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "classifier.add(Convolution2D(32,3,3,input_shape=(64,64,3),activation = 'relu'))\n",
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dense(units=1,activation='relu'))\n",
    "classifier.add(Dense(units=5,activation='relu'))\n",
    "classifier.add(Dense(units=10,activation='sigmoid'))\n",
    "classifier.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating computational array from Image data from directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "train_data_generator = ImageDataGenerator(rescale=1./255,shear_range=0.2,zoom_range=0.2,horizontal_flip=True)\n",
    "test_data_generator =ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### appending path of  train and test images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_train=pd.read_csv(\"/home/admin3/Documents/MyDoc/data_sets/Identify-numbers/Train_UQcUa52/train.csv\")\n",
    "class_test =pd.read_csv(\"/home/admin3/Documents/MyDoc/data_sets/Identify-numbers/Test_fCbTej3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_of_train_image = \"/home/admin3/Documents/MyDoc/data_sets/Identify-numbers/Train_UQcUa52/Images/train/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_file_with_classes(path,data_set,column_name):\n",
    "    return [path+image for image in data_set[column_name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_train[\"full_path\"]=image_file_with_classes(path_of_train_image,class_train,'filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_train.drop(['filename'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>full_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>/home/admin3/Documents/MyDoc/data_sets/Identif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>/home/admin3/Documents/MyDoc/data_sets/Identif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>/home/admin3/Documents/MyDoc/data_sets/Identif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>/home/admin3/Documents/MyDoc/data_sets/Identif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>/home/admin3/Documents/MyDoc/data_sets/Identif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48995</th>\n",
       "      <td>2</td>\n",
       "      <td>/home/admin3/Documents/MyDoc/data_sets/Identif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48996</th>\n",
       "      <td>4</td>\n",
       "      <td>/home/admin3/Documents/MyDoc/data_sets/Identif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48997</th>\n",
       "      <td>9</td>\n",
       "      <td>/home/admin3/Documents/MyDoc/data_sets/Identif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48998</th>\n",
       "      <td>3</td>\n",
       "      <td>/home/admin3/Documents/MyDoc/data_sets/Identif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48999</th>\n",
       "      <td>0</td>\n",
       "      <td>/home/admin3/Documents/MyDoc/data_sets/Identif...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                          full_path\n",
       "0          4  /home/admin3/Documents/MyDoc/data_sets/Identif...\n",
       "1          9  /home/admin3/Documents/MyDoc/data_sets/Identif...\n",
       "2          1  /home/admin3/Documents/MyDoc/data_sets/Identif...\n",
       "3          7  /home/admin3/Documents/MyDoc/data_sets/Identif...\n",
       "4          3  /home/admin3/Documents/MyDoc/data_sets/Identif...\n",
       "...      ...                                                ...\n",
       "48995      2  /home/admin3/Documents/MyDoc/data_sets/Identif...\n",
       "48996      4  /home/admin3/Documents/MyDoc/data_sets/Identif...\n",
       "48997      9  /home/admin3/Documents/MyDoc/data_sets/Identif...\n",
       "48998      3  /home/admin3/Documents/MyDoc/data_sets/Identif...\n",
       "48999      0  /home/admin3/Documents/MyDoc/data_sets/Identif...\n",
       "\n",
       "[49000 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_train=class_train.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = class_train[:37000]\n",
    "test_df = class_train[37000:]\n",
    "test_df =test_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 37000 validated image filenames belonging to 10 classes.\n",
      "Found 12000 validated image filenames belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = train_data_generator.flow_from_dataframe(dataframe=train_df,\n",
    "                                                        x_col='full_path',\n",
    "                                                        target_size=(64,64),\n",
    "                                                        batch_size=32,\n",
    "                                                        class_mode='categorical',\n",
    "                                                        y_col='label')\n",
    "test_set = train_data_generator.flow_from_dataframe(dataframe=test_df,\n",
    "                                                        x_col='full_path',\n",
    "                                                        target_size=(64,64),\n",
    "                                                        batch_size=32,\n",
    "                                                        class_mode='categorical',\n",
    "                                                        y_col='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_of_test_image = \"/home/admin3/Documents/MyDoc/data_sets/Identify-numbers/Train_UQcUa52/Images/test/\"\n",
    "class_test[\"full_path\"]=image_file_with_classes(path_of_test_image,class_test,'filename')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:From /home/admin3/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "1157/1157 [==============================] - 52s 45ms/step - loss: 2.0972 - acc: 0.1758 - val_loss: 2.0012 - val_acc: 0.1961\n",
      "Epoch 2/40\n",
      "1157/1157 [==============================] - 52s 45ms/step - loss: 1.9467 - acc: 0.2141 - val_loss: 1.9119 - val_acc: 0.2421\n",
      "Epoch 3/40\n",
      "1157/1157 [==============================] - 53s 46ms/step - loss: 1.8794 - acc: 0.2394 - val_loss: 1.8579 - val_acc: 0.2376\n",
      "Epoch 4/40\n",
      "1157/1157 [==============================] - 53s 45ms/step - loss: 1.8341 - acc: 0.2683 - val_loss: 1.8079 - val_acc: 0.2841\n",
      "Epoch 5/40\n",
      "1157/1157 [==============================] - 53s 46ms/step - loss: 1.7876 - acc: 0.2929 - val_loss: 1.7772 - val_acc: 0.3001\n",
      "Epoch 6/40\n",
      "1157/1157 [==============================] - 52s 45ms/step - loss: 1.7483 - acc: 0.3100 - val_loss: 1.7344 - val_acc: 0.3054\n",
      "Epoch 7/40\n",
      "1157/1157 [==============================] - 52s 45ms/step - loss: 1.7165 - acc: 0.3171 - val_loss: 1.7069 - val_acc: 0.3192\n",
      "Epoch 8/40\n",
      "1157/1157 [==============================] - 52s 45ms/step - loss: 1.6817 - acc: 0.3317 - val_loss: 1.6846 - val_acc: 0.3244\n",
      "Epoch 9/40\n",
      "1157/1157 [==============================] - 52s 45ms/step - loss: 1.6569 - acc: 0.3387 - val_loss: 1.6721 - val_acc: 0.3362\n",
      "Epoch 10/40\n",
      "1157/1157 [==============================] - 53s 46ms/step - loss: 1.6362 - acc: 0.3472 - val_loss: 1.6457 - val_acc: 0.3338\n",
      "Epoch 11/40\n",
      "1157/1157 [==============================] - 53s 46ms/step - loss: 1.6143 - acc: 0.3538 - val_loss: 1.6145 - val_acc: 0.3541\n",
      "Epoch 12/40\n",
      "1157/1157 [==============================] - 54s 47ms/step - loss: 1.6001 - acc: 0.3584 - val_loss: 1.6004 - val_acc: 0.3545\n",
      "Epoch 13/40\n",
      "1157/1157 [==============================] - 53s 45ms/step - loss: 1.5804 - acc: 0.3648 - val_loss: 1.5966 - val_acc: 0.3500\n",
      "Epoch 14/40\n",
      "1157/1157 [==============================] - 53s 46ms/step - loss: 1.5639 - acc: 0.3708 - val_loss: 1.5836 - val_acc: 0.3535\n",
      "Epoch 15/40\n",
      "1157/1157 [==============================] - 52s 45ms/step - loss: 1.5543 - acc: 0.3689 - val_loss: 1.5507 - val_acc: 0.3785\n",
      "Epoch 16/40\n",
      "1157/1157 [==============================] - 53s 46ms/step - loss: 1.5382 - acc: 0.3747 - val_loss: 1.5358 - val_acc: 0.3798\n",
      "Epoch 17/40\n",
      "1157/1157 [==============================] - 52s 45ms/step - loss: 1.5334 - acc: 0.3767 - val_loss: 1.5333 - val_acc: 0.3708\n",
      "Epoch 18/40\n",
      "1157/1157 [==============================] - 52s 45ms/step - loss: 1.5146 - acc: 0.3778 - val_loss: 1.5194 - val_acc: 0.3843\n",
      "Epoch 19/40\n",
      "1157/1157 [==============================] - 52s 45ms/step - loss: 1.5077 - acc: 0.3792 - val_loss: 1.5087 - val_acc: 0.3717\n",
      "Epoch 20/40\n",
      "1157/1157 [==============================] - 53s 45ms/step - loss: 1.4987 - acc: 0.3774 - val_loss: 1.5394 - val_acc: 0.3517\n",
      "Epoch 21/40\n",
      "1157/1157 [==============================] - 53s 46ms/step - loss: 1.4882 - acc: 0.3843 - val_loss: 1.5008 - val_acc: 0.3873\n",
      "Epoch 22/40\n",
      "1157/1157 [==============================] - 54s 46ms/step - loss: 1.4832 - acc: 0.3866 - val_loss: 1.4755 - val_acc: 0.3941\n",
      "Epoch 23/40\n",
      "1157/1157 [==============================] - 57s 49ms/step - loss: 1.4751 - acc: 0.3897 - val_loss: 1.4856 - val_acc: 0.3859\n",
      "Epoch 24/40\n",
      "1157/1157 [==============================] - 59s 51ms/step - loss: 1.4635 - acc: 0.3996 - val_loss: 1.4733 - val_acc: 0.3987\n",
      "Epoch 25/40\n",
      "1157/1157 [==============================] - 57s 49ms/step - loss: 1.4599 - acc: 0.4026 - val_loss: 1.4619 - val_acc: 0.4004\n",
      "Epoch 26/40\n",
      "1157/1157 [==============================] - 56s 48ms/step - loss: 1.4505 - acc: 0.4047 - val_loss: 1.4679 - val_acc: 0.4128\n",
      "Epoch 27/40\n",
      "1157/1157 [==============================] - 55s 48ms/step - loss: 1.4434 - acc: 0.4116 - val_loss: 1.4512 - val_acc: 0.4046\n",
      "Epoch 28/40\n",
      "1157/1157 [==============================] - 52s 45ms/step - loss: 1.4331 - acc: 0.4135 - val_loss: 1.4423 - val_acc: 0.4016\n",
      "Epoch 29/40\n",
      "1157/1157 [==============================] - 52s 45ms/step - loss: 1.4321 - acc: 0.4142 - val_loss: 1.4339 - val_acc: 0.4194\n",
      "Epoch 30/40\n",
      "1157/1157 [==============================] - 52s 45ms/step - loss: 1.4245 - acc: 0.4201 - val_loss: 1.4251 - val_acc: 0.4249\n",
      "Epoch 31/40\n",
      "1157/1157 [==============================] - 52s 45ms/step - loss: 1.4229 - acc: 0.4196 - val_loss: 1.4360 - val_acc: 0.4251\n",
      "Epoch 32/40\n",
      "1157/1157 [==============================] - 51s 44ms/step - loss: 1.4157 - acc: 0.4212 - val_loss: 1.4502 - val_acc: 0.4019\n",
      "Epoch 33/40\n",
      "1157/1157 [==============================] - 51s 44ms/step - loss: 1.4091 - acc: 0.4270 - val_loss: 1.4262 - val_acc: 0.4227\n",
      "Epoch 34/40\n",
      "1157/1157 [==============================] - 52s 45ms/step - loss: 1.4124 - acc: 0.4251 - val_loss: 1.4224 - val_acc: 0.4268\n",
      "Epoch 35/40\n",
      "1157/1157 [==============================] - 54s 47ms/step - loss: 1.4075 - acc: 0.4284 - val_loss: 1.4102 - val_acc: 0.4230\n",
      "Epoch 36/40\n",
      "1157/1157 [==============================] - 57s 49ms/step - loss: 1.4011 - acc: 0.4331 - val_loss: 1.4094 - val_acc: 0.4317\n",
      "Epoch 37/40\n",
      "1157/1157 [==============================] - 53s 46ms/step - loss: 1.4074 - acc: 0.4322 - val_loss: 1.4058 - val_acc: 0.4304\n",
      "Epoch 38/40\n",
      "1157/1157 [==============================] - 52s 45ms/step - loss: 1.3967 - acc: 0.4361 - val_loss: 1.4345 - val_acc: 0.4320\n",
      "Epoch 39/40\n",
      "1157/1157 [==============================] - 53s 46ms/step - loss: 1.4012 - acc: 0.4366 - val_loss: 1.4036 - val_acc: 0.4291\n",
      "Epoch 40/40\n",
      "1157/1157 [==============================] - 51s 44ms/step - loss: 1.3939 - acc: 0.4429 - val_loss: 1.3983 - val_acc: 0.4411\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f61f01b06d8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(training_set,epochs=40,batch_size=1,validation_data = test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### predicting output based on model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "images = image_file_with_classes(path_of_test_image,class_test,'filename')\n",
    "images = [image.load_img(path, target_size = (64,64,1)) for path in images]\n",
    "images = [image.img_to_array(img) for img in images]\n",
    "images = [np.expand_dims(img, axis = 0) for img in images]\n",
    "pred_img = np.array([classifier.predict_classes(img) for img in images])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f61eb93f6d8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_path=path_of_test_image+'59111.png'\n",
    "img = image.load_img(image_path, target_size=(64, 64,1))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "(unique, counts) = np.unique(pred_img, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5 6 7 8 9] \n",
      " [ 6121     1     6 14849     2     6     3     4     8]\n"
     ]
    }
   ],
   "source": [
    "print(unique[:],\"\\n\",counts[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = class_train['full_path'].tolist()\n",
    "train_images = [image.load_img(path, target_size = (64,64,1)) for path in train_images]\n",
    "train_images = [image.img_to_array(img) for img in train_images]\n",
    "train_images = [np.expand_dims(img, axis = 0) for img in train_images]\n",
    "train_pred_img = np.array([classifier.predict_classes(img) for img in train_images])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "(unique_t, counts_t) = np.unique(train_pred_img, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5 6 7 8 9] \n",
      " [14485     5     7 34461     3     9     6     8    16]\n"
     ]
    }
   ],
   "source": [
    "print(unique_t[:],\"\\n\",counts_t[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0' '1' '2' '3' '4' '5' '6' '7' '8' '9'] \n",
      " [4832 5514 4893 4999 4777 4419 4813 5105 4777 4871]\n"
     ]
    }
   ],
   "source": [
    "true_train=class_train['label'].values\n",
    "(unique_true, counts_true) = np.unique(true_train, return_counts=True)\n",
    "print(unique_true[:],\"\\n\",counts_true[:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
