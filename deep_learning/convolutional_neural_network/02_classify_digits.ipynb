{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "problem statement :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We need to identify the digit in given images. We have total 70,000 images, out of which 49,000 are part of train images with the label of digit and rest 21,000 images are unlabeled (known as test images). Now, We need to identify the digit for test images. Public and Private split for test images are 40:60 and evaluation metric of this challenge is accuracy\n",
    "    https://drive.google.com/open?id=1-TavdjU2ohg5T6ZN1KUiWE9BNWu9_vhi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin3/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/admin3/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/admin3/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/admin3/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/admin3/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/admin3/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#importing necessary library\n",
    "#importing matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "#importing seaborn\n",
    "import seaborn as sea\n",
    "#importing pandas \n",
    "import pandas as pd\n",
    "#importing numpy\n",
    "import numpy as np\n",
    "\n",
    "# importing my custom library file methods\n",
    "import sys\n",
    "sys.path.append('/home/admin3/ml_with_phoenix/deep_learning/lib_and_pkl_files/')\n",
    "from ipynb.fs.full.library import *\n",
    "\n",
    "#importing job-lib\n",
    "import joblib\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Convolution2D,MaxPooling2D,Flatten\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making CNN model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/admin3/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "classifier = Sequential()\n",
    "classifier.add(Convolution2D(32,3,3,input_shape=(64,64,3),activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "classifier.add(Convolution2D(32,3,3,input_shape=(64,64,3),activation = 'relu'))\n",
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dense(units=1,activation='relu'))\n",
    "classifier.add(Dense(units=10,activation='sigmoid'))\n",
    "classifier.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating computational array from Image data from directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "train_data_generator = ImageDataGenerator(rescale=1./255,shear_range=0.2,zoom_range=0.2,horizontal_flip=True)\n",
    "test_data_generator =ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### obtaining Train and Test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_train=pd.read_csv(\"/home/admin3/Documents/MyDoc/data_sets/Identify-numbers/Train_UQcUa52/train.csv\")\n",
    "class_test =pd.read_csv(\"/home/admin3/Documents/MyDoc/data_sets/Identify-numbers/Test_fCbTej3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_of_train_image = \"/home/admin3/Documents/MyDoc/data_sets/Identify-numbers/Train_UQcUa52/Images/train/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_file_with_classes(path,data_set,column_name):\n",
    "    return [path+image for image in data_set[column_name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_train[\"full_path\"]=image_file_with_classes(path_of_train_image,class_train,'filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_train.drop(['filename'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>full_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>/home/admin3/Documents/MyDoc/data_sets/Identif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>/home/admin3/Documents/MyDoc/data_sets/Identif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>/home/admin3/Documents/MyDoc/data_sets/Identif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>/home/admin3/Documents/MyDoc/data_sets/Identif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>/home/admin3/Documents/MyDoc/data_sets/Identif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48995</th>\n",
       "      <td>2</td>\n",
       "      <td>/home/admin3/Documents/MyDoc/data_sets/Identif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48996</th>\n",
       "      <td>4</td>\n",
       "      <td>/home/admin3/Documents/MyDoc/data_sets/Identif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48997</th>\n",
       "      <td>9</td>\n",
       "      <td>/home/admin3/Documents/MyDoc/data_sets/Identif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48998</th>\n",
       "      <td>3</td>\n",
       "      <td>/home/admin3/Documents/MyDoc/data_sets/Identif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48999</th>\n",
       "      <td>0</td>\n",
       "      <td>/home/admin3/Documents/MyDoc/data_sets/Identif...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                          full_path\n",
       "0          4  /home/admin3/Documents/MyDoc/data_sets/Identif...\n",
       "1          9  /home/admin3/Documents/MyDoc/data_sets/Identif...\n",
       "2          1  /home/admin3/Documents/MyDoc/data_sets/Identif...\n",
       "3          7  /home/admin3/Documents/MyDoc/data_sets/Identif...\n",
       "4          3  /home/admin3/Documents/MyDoc/data_sets/Identif...\n",
       "...      ...                                                ...\n",
       "48995      2  /home/admin3/Documents/MyDoc/data_sets/Identif...\n",
       "48996      4  /home/admin3/Documents/MyDoc/data_sets/Identif...\n",
       "48997      9  /home/admin3/Documents/MyDoc/data_sets/Identif...\n",
       "48998      3  /home/admin3/Documents/MyDoc/data_sets/Identif...\n",
       "48999      0  /home/admin3/Documents/MyDoc/data_sets/Identif...\n",
       "\n",
       "[49000 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_train=class_train.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 49000 validated image filenames belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = train_data_generator.flow_from_dataframe(dataframe=class_train,\n",
    "                                                        x_col='full_path',\n",
    "                                                        target_size=(64,64),\n",
    "                                                        batch_size=32,\n",
    "                                                        class_mode='categorical',\n",
    "                                                        y_col='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_of_test_image = \"/home/admin3/Documents/MyDoc/data_sets/Identify-numbers/Train_UQcUa52/Images/test/\"\n",
    "class_test[\"full_path\"]=image_file_with_classes(path_of_test_image,class_test,'filename')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:From /home/admin3/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "1532/1532 [==============================] - 52s 34ms/step - loss: 1.9922 - acc: 0.2059\n",
      "Epoch 2/50\n",
      "1532/1532 [==============================] - 51s 33ms/step - loss: 1.8417 - acc: 0.2150\n",
      "Epoch 3/50\n",
      "1532/1532 [==============================] - 51s 34ms/step - loss: 1.7917 - acc: 0.2194\n",
      "Epoch 4/50\n",
      "1532/1532 [==============================] - 51s 34ms/step - loss: 1.7661 - acc: 0.2252\n",
      "Epoch 5/50\n",
      "1532/1532 [==============================] - 51s 33ms/step - loss: 1.7515 - acc: 0.2284\n",
      "Epoch 6/50\n",
      "1532/1532 [==============================] - 54s 35ms/step - loss: 1.7435 - acc: 0.2329 1s -\n",
      "Epoch 7/50\n",
      "1532/1532 [==============================] - 54s 35ms/step - loss: 1.7335 - acc: 0.2394\n",
      "Epoch 8/50\n",
      "1532/1532 [==============================] - 54s 35ms/step - loss: 1.7246 - acc: 0.2591\n",
      "Epoch 9/50\n",
      "1532/1532 [==============================] - 52s 34ms/step - loss: 1.7004 - acc: 0.2979\n",
      "Epoch 10/50\n",
      "1532/1532 [==============================] - 51s 34ms/step - loss: 1.6621 - acc: 0.3294\n",
      "Epoch 11/50\n",
      "1532/1532 [==============================] - 52s 34ms/step - loss: 1.6274 - acc: 0.3503\n",
      "Epoch 12/50\n",
      "1532/1532 [==============================] - 51s 34ms/step - loss: 1.5880 - acc: 0.3790\n",
      "Epoch 13/50\n",
      "1532/1532 [==============================] - 51s 34ms/step - loss: 1.5613 - acc: 0.3925\n",
      "Epoch 14/50\n",
      "1532/1532 [==============================] - 51s 33ms/step - loss: 1.5431 - acc: 0.3971\n",
      "Epoch 15/50\n",
      "1532/1532 [==============================] - 52s 34ms/step - loss: 1.5293 - acc: 0.4038\n",
      "Epoch 16/50\n",
      "1532/1532 [==============================] - 52s 34ms/step - loss: 1.5141 - acc: 0.4035\n",
      "Epoch 17/50\n",
      "1532/1532 [==============================] - 52s 34ms/step - loss: 1.5075 - acc: 0.4053\n",
      "Epoch 18/50\n",
      "1532/1532 [==============================] - 54s 35ms/step - loss: 1.4955 - acc: 0.4088\n",
      "Epoch 19/50\n",
      "1532/1532 [==============================] - 52s 34ms/step - loss: 1.4893 - acc: 0.4066\n",
      "Epoch 20/50\n",
      "1532/1532 [==============================] - 52s 34ms/step - loss: 1.4819 - acc: 0.4088\n",
      "Epoch 21/50\n",
      "1532/1532 [==============================] - 52s 34ms/step - loss: 1.4803 - acc: 0.4050\n",
      "Epoch 22/50\n",
      "1532/1532 [==============================] - 52s 34ms/step - loss: 1.4694 - acc: 0.4081\n",
      "Epoch 23/50\n",
      "1532/1532 [==============================] - 52s 34ms/step - loss: 1.4702 - acc: 0.4108\n",
      "Epoch 24/50\n",
      "1532/1532 [==============================] - 52s 34ms/step - loss: 1.4614 - acc: 0.4073\n",
      "Epoch 25/50\n",
      "1532/1532 [==============================] - 52s 34ms/step - loss: 1.4551 - acc: 0.4128\n",
      "Epoch 26/50\n",
      "1532/1532 [==============================] - 52s 34ms/step - loss: 1.4581 - acc: 0.3986\n",
      "Epoch 27/50\n",
      "1532/1532 [==============================] - 52s 34ms/step - loss: 1.4527 - acc: 0.3948\n",
      "Epoch 28/50\n",
      "1532/1532 [==============================] - 52s 34ms/step - loss: 1.4478 - acc: 0.3970\n",
      "Epoch 29/50\n",
      "1532/1532 [==============================] - 52s 34ms/step - loss: 1.4476 - acc: 0.3947\n",
      "Epoch 30/50\n",
      "1532/1532 [==============================] - 52s 34ms/step - loss: 1.4401 - acc: 0.3960\n",
      "Epoch 31/50\n",
      "1532/1532 [==============================] - 58s 38ms/step - loss: 1.4395 - acc: 0.3973\n",
      "Epoch 32/50\n",
      "1532/1532 [==============================] - 53s 35ms/step - loss: 1.4361 - acc: 0.3980\n",
      "Epoch 33/50\n",
      "1532/1532 [==============================] - 54s 35ms/step - loss: 1.4359 - acc: 0.3965\n",
      "Epoch 34/50\n",
      "1532/1532 [==============================] - 54s 35ms/step - loss: 1.4358 - acc: 0.3981\n",
      "Epoch 35/50\n",
      "1532/1532 [==============================] - 53s 35ms/step - loss: 1.4315 - acc: 0.4080\n",
      "Epoch 36/50\n",
      "1532/1532 [==============================] - 52s 34ms/step - loss: 1.4327 - acc: 0.4096\n",
      "Epoch 37/50\n",
      "1532/1532 [==============================] - 53s 35ms/step - loss: 1.4259 - acc: 0.4021\n",
      "Epoch 38/50\n",
      "1532/1532 [==============================] - 52s 34ms/step - loss: 1.4277 - acc: 0.4046\n",
      "Epoch 39/50\n",
      "1532/1532 [==============================] - 52s 34ms/step - loss: 1.4221 - acc: 0.4021\n",
      "Epoch 40/50\n",
      "1532/1532 [==============================] - 57s 37ms/step - loss: 1.4255 - acc: 0.4080\n",
      "Epoch 41/50\n",
      "1532/1532 [==============================] - 57s 37ms/step - loss: 1.4203 - acc: 0.4104\n",
      "Epoch 42/50\n",
      "1532/1532 [==============================] - 58s 38ms/step - loss: 1.4164 - acc: 0.4099\n",
      "Epoch 43/50\n",
      "1532/1532 [==============================] - 58s 38ms/step - loss: 1.4178 - acc: 0.4119\n",
      "Epoch 44/50\n",
      "1532/1532 [==============================] - 55s 36ms/step - loss: 1.4132 - acc: 0.4159\n",
      "Epoch 45/50\n",
      "1532/1532 [==============================] - 55s 36ms/step - loss: 1.4139 - acc: 0.4177\n",
      "Epoch 46/50\n",
      "1532/1532 [==============================] - 52s 34ms/step - loss: 1.4082 - acc: 0.4151\n",
      "Epoch 47/50\n",
      "1532/1532 [==============================] - 52s 34ms/step - loss: 1.4148 - acc: 0.4280\n",
      "Epoch 48/50\n",
      "1532/1532 [==============================] - 52s 34ms/step - loss: 1.4044 - acc: 0.4280\n",
      "Epoch 49/50\n",
      "1532/1532 [==============================] - 52s 34ms/step - loss: 1.4073 - acc: 0.4292\n",
      "Epoch 50/50\n",
      "1532/1532 [==============================] - 52s 34ms/step - loss: 1.4089 - acc: 0.4290\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f52006e2b70>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(training_set,epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "images = image_file_with_classes(path_of_test_image,class_test,'filename')\n",
    "images = [image.load_img(path, target_size = (64,64,1)) for path in images]\n",
    "images = [image.img_to_array(img) for img in images]\n",
    "images = [np.expand_dims(img, axis = 0) for img in images]\n",
    "pred_img = np.array([classifier.predict_classes(img) for img in images])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f518b0e6d30>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_path=path_of_test_image+'59111.png'\n",
    "img = image.load_img(image_path, target_size=(64, 64,1))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "(unique, counts) = np.unique(pred_img, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 4 6 7 9] \n",
      " [ 7313 13609    11    29    16    10    12]\n"
     ]
    }
   ],
   "source": [
    "print(unique[:],\"\\n\",counts[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = class_train['full_path'].tolist()\n",
    "train_images = [image.load_img(path, target_size = (64,64,1)) for path in train_images]\n",
    "train_images = [image.img_to_array(img) for img in train_images]\n",
    "train_images = [np.expand_dims(img, axis = 0) for img in train_images]\n",
    "train_pred_img = np.array([classifier.predict_classes(img) for img in train_images])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "(unique_t, counts_t) = np.unique(train_pred_img, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 4 6 7 9] \n",
      " [17136 31684    29    55    38    31    27]\n"
     ]
    }
   ],
   "source": [
    "print(unique_t[:],\"\\n\",counts_t[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0' '1' '2' '3' '4' '5' '6' '7' '8' '9'] \n",
      " [4832 5514 4893 4999 4777 4419 4813 5105 4777 4871]\n"
     ]
    }
   ],
   "source": [
    "true_train=class_train['label'].values\n",
    "(unique_true, counts_true) = np.unique(true_train, return_counts=True)\n",
    "print(unique_true[:],\"\\n\",counts_true[:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
